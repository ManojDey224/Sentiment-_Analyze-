# -*- coding: utf-8 -*-
"""Sentiment Analyze.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15RGGIeGTeB9U32Wm8HZ3t9yNjPghHELs
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Impoting all the necessary library required
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input,Dense,LSTM,GlobalMaxPooling1D,Embedding
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report,confusion_matrix
import re,string,nltk
import tensorflow as tf
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer,WordNetLemmatizer
from tensorflow.keras.callbacks import EarlyStopping

df2=pd.read_csv('Reddit_Data.csv')
df1=pd.read_csv('Twitter_Data.csv')

df1.head()
df2.head()

#renaming the columns
df2.columns=['messages','labels']
df1.columns=['messages','labels']

df1.head()

df2.head()

df = pd.concat([df1, df2], ignore_index = True)

df['messages'].iloc[0] #iloc is used to select a row

df['messages'].iloc[100]

df.head()

df.tail()

df['length']=df['messages'].str.len()

sns.set_style('whitegrid')
df['length'].plot(kind='hist',bins=100,)

df.info()

df.describe().transpose()

#from the describe function we got to know that the longest message is of length 8665
df[df['length']==8665]['messages'].iloc[0]

# mapping the labels to their respective meanings
df['labels']=df['labels'].map({-1:'negative',0:'neutral',1:'positive'})

#checking the null values in the dataset
df.isnull().sum()

#removing the null value witg dropna()
df=df.dropna()

len(df['messages'])

#cleaning the data from unnecessary data

#replacing url with string URL
def replace_url(text):
    return re.sub('https?:\/\/\S*|www\.\S+','URL',text)

#removing html
def remove_html(text):
    return re.sub('<.*?>','',text)

#replacing mentions with string user
def replace_mentions(text):
    return re.sub('@\S*','user',text,flags=re.IGNORECASE)

#replacing numbers with string number
def replace_num(text):
    return re.sub('^[+-]*?\d{1,3}[- ]*?\d{1,10}|\d{10}','NUMBER',text)

#replacing <3 with sring heart
def replace_heart(text):
    return re.sub('<3','HEART', text)

#removing alphanumeric characters eg-XYZ123ABC
def remove_alphanumeric(text):
    return re.sub('\w*\d+\w*','',text)

#removing all english stopwords
def remove_stopwords(text):
    text = ' '.join([word for word in text.split() if word not in stopwords.words("english")])
    return text

#removing punctuations
def remove_punctuations(text):
    text=''.join([word for word in text if word not in string.punctuation])
    return text

#reducing words to thier root form
def lemmatization(text):
    lm= WordNetLemmatizer()
    text = ' '.join([lm.lemmatize(word, pos='v') for word in text.split()])
    return text

def clean_text(text):
    text=str(text).lower()
    text = replace_url(text)
    text = remove_html(text)
    text = replace_mentions(text)
    text = replace_num(text)
    text = replace_heart(text)
    text = remove_alphanumeric(text)
    text = remove_stopwords(text)
    text=remove_punctuations(text)
    #text=stemming(text)
    text=lemmatization(text)
    return text

!pip install nltk

nltk.download('wordnet')
nltk.download('stopwords')

import nltk
from nltk.corpus import wordnet

df['messages1']=df['messages'].apply(clean_text)

df['messages1']

#splitting the data for training and testing
X=df['messages1'].values
y = pd.get_dummies(df['labels']).values
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33)

X_train.shape

#converting the messages into tokens and then converting them into matrix of integers
Max_vocab_size=50000
tokenizer=Tokenizer(num_words=Max_vocab_size)
tokenizer.fit_on_texts(X_train)
sequence_Xtrain=tokenizer.texts_to_sequences(X_train)
sequence_Xtest=tokenizer.texts_to_sequences(X_test)

!pip install tokenizer
import tokenizer
from tensorflow.keras.preprocessing.text import Tokenizer

print(type(tokenizer))

tokenizer = Tokenizer()

print(tokenizer.word_index)

V = len(tokenizer.word_index)

tokenizer.word_index

#padding the integer matrix with zero so that all the data have the same length
data_train=pad_sequences(sequence_Xtrain)

T=data_train.shape[1]
print(T)

data_test=pad_sequences(sequence_Xtest,maxlen=T)

data_train.shape

data_test.shape

D=20 #No of features you want
early_stop=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=25)
i=Input(shape=(T,))
x=Embedding(V+1,D)(i)
x=LSTM(128,dropout=(0.2))(x)
x=Dense(3,activation='softmax')(x)
model=Model(i,x)
model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])
history=model.fit(data_train,y_train,batch_size=128,validation_data=(data_test,y_test),epochs=5,callbacks=[early_stop])

from tensorflow.keras.layers import Input, Embedding, LSTM, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import CategoricalCrossentropy
from tensorflow.keras.metrics import Accuracy
from tensorflow.keras.callbacks import EarlyStopping

# Add any other required modules here

model.summary()

#evaluating the model performance
model.evaluate(data_test,y_test)

losses=pd.DataFrame(history.history)
losses.plot()

plt.plot(history.history['val_accuracy'],label='val_acc')
plt.plot(history.history['accuracy'],label='accuracy')
plt.legend()

predictions=model.predict(data_test).argmax(axis=1)

print(classification_report(y_test.argmax(axis=1),predictions))

model.save('model#1.h5')